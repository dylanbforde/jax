{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to install JAX with TPU support and other dependencies. **After running this cell, you must restart the kernel** for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81166b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
      "Requirement already satisfied: jax>=0.4.30 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]>=0.4.30) (0.7.2)\n",
      "Requirement already satisfied: jaxlib<=0.7.2,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.30->jax[tpu]>=0.4.30) (0.7.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.30->jax[tpu]>=0.4.30) (0.5.4)\n",
      "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.30->jax[tpu]>=0.4.30) (2.0.2)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.30->jax[tpu]>=0.4.30) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.30->jax[tpu]>=0.4.30) (1.16.3)\n",
      "Requirement already satisfied: libtpu==0.0.23 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]>=0.4.30) (0.0.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jax[tpu]>=0.4.30) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]>=0.4.30) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]>=0.4.30) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]>=0.4.30) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]>=0.4.30) (2026.1.4)\n",
      "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax) (2.0.2)\n",
      "Requirement already satisfied: jax>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from flax) (0.7.2)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.2)\n",
      "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax) (0.11.32)\n",
      "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.81)\n",
      "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (14.3.2)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.3)\n",
      "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax) (1.4.0)\n",
      "Requirement already satisfied: jaxlib>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.7.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (1.16.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
      "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.13.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (25.1.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (6.33.5)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (4.15.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (3.20.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (5.9.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.23.0)\n",
      "Installation complete. Please restart the runtime/kernel now.\n"
     ]
    }
   ],
   "source": [
    "# Install JAX for TPU\n",
    "!pip install \"jax[tpu]>=0.4.30\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install flax optax\n",
    "\n",
    "import os\n",
    "print(\"Installation complete. Please restart the runtime/kernel now.\")\n",
    "# os.kill(os.getpid(), 9) # Uncomment to automatically restart in some environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5e1f0",
   "metadata": {},
   "source": [
    "# Pallas Sinkhorn Implementation\n",
    "\n",
    "This notebook contains the optimized Pallas implementation of the Sinkhorn-Knopp algorithm, including a custom VJP for memory-efficient backward passes.\n",
    "\n",
    "Running this notebook will write the implementation to `pallas_sinkhorn.py` used by the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a445490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pallas_sinkhorn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pallas_sinkhorn.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "\n",
    "try:\n",
    "    from jax.experimental.pallas import tpu as pltpu\n",
    "except ImportError:\n",
    "    pltpu = None\n",
    "import functools\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def resolve_compiler_params(dim_semantics):\n",
    "    if pltpu is None:\n",
    "        return None\n",
    "    if hasattr(pltpu, \"CompilerParams\"):\n",
    "        return pltpu.CompilerParams(dimension_semantics=dim_semantics)\n",
    "    elif hasattr(pltpu, \"TPUCompilerParams\"):\n",
    "        return pltpu.TPUCompilerParams(dimension_semantics=dim_semantics)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Forward Kernel Tile Level\n",
    "def tile_sinkhorn_kernel(q_ref, k_ref, v_ref, o_ref, u_ref, v_pot_ref, *, n_iters):\n",
    "    # Load 128x64 tiles directly into VMEM\n",
    "    q = q_ref[...]\n",
    "    k = k_ref[...]\n",
    "    v = v_ref[...]\n",
    "\n",
    "    D = q.shape[-1]\n",
    "\n",
    "    # 1. Local Logits (In-SRAM)\n",
    "    logits = jnp.dot(q, k.T) / jnp.sqrt(D)\n",
    "\n",
    "    # Initialize Potentials\n",
    "    u_acc = jnp.zeros((logits.shape[0], 8), dtype=logits.dtype)\n",
    "    v_acc = jnp.zeros((logits.shape[1], 8), dtype=logits.dtype)\n",
    "\n",
    "    # 2. Local Sinkhorn Iterations\n",
    "    log_alpha = logits\n",
    "    for _ in range(n_iters):\n",
    "        # Row norm\n",
    "        max_r = jnp.max(log_alpha, axis=1, keepdims=True)\n",
    "        correction_r = max_r + jnp.log(\n",
    "            jnp.sum(jnp.exp(log_alpha - max_r), axis=1, keepdims=True)\n",
    "        )\n",
    "        # Broadcast correction_r explicitly for TPU compilation\n",
    "        u_acc = u_acc - jnp.broadcast_to(correction_r, u_acc.shape)\n",
    "\n",
    "        # Col norm\n",
    "        max_c = jnp.max(log_alpha, axis=0, keepdims=True)\n",
    "        correction_c = max_c + jnp.log(\n",
    "            jnp.sum(jnp.exp(log_alpha - max_c), axis=0, keepdims=True)\n",
    "        )\n",
    "        log_alpha = log_alpha - correction_c\n",
    "        \n",
    "        # Broadcast correction_c explicitly for TPU compilation\n",
    "        v_acc = v_acc - jnp.broadcast_to(correction_c.T, v_acc.shape)\n",
    "\n",
    "    # 3. Multiply by V\n",
    "    o_ref[...] = jnp.dot(jnp.exp(log_alpha), v)\n",
    "    u_ref[...] = u_acc\n",
    "    v_pot_ref[...] = v_acc\n",
    "\n",
    "\n",
    "def pallas_sinkhorn_fwd(Q, K, V, n_iters=20):\n",
    "    # Q, K, V are [B, H, N, D]\n",
    "    B, H, L, D = Q.shape\n",
    "    block_size = 128\n",
    "    num_blocks = L // block_size\n",
    "    head_dim = D\n",
    "\n",
    "    # Chunk into tiles for Flash loading\n",
    "    q_blocks = Q.reshape(B, H, num_blocks, block_size, head_dim)\n",
    "    k_blocks = K.reshape(B, H, num_blocks, block_size, head_dim)\n",
    "    v_blocks = V.reshape(B, H, num_blocks, block_size, head_dim)\n",
    "\n",
    "    # Define tile computation\n",
    "    def compute_tile(q_tile, k_tile, v_tile):\n",
    "        out_shape = jax.ShapeDtypeStruct((block_size, head_dim), Q.dtype)\n",
    "        u_shape = jax.ShapeDtypeStruct((block_size, 8), Q.dtype)\n",
    "        v_shape = jax.ShapeDtypeStruct((block_size, 8), Q.dtype)\n",
    "\n",
    "        is_cpu = jax.devices()[0].platform == \"cpu\"\n",
    "        kw = {\"interpret\": True} if is_cpu else {}\n",
    "\n",
    "        # Use pallas_call for the tile computation\n",
    "        return pl.pallas_call(\n",
    "            functools.partial(tile_sinkhorn_kernel, n_iters=n_iters),\n",
    "            out_shape=(out_shape, u_shape, v_shape),\n",
    "            grid=(),  # Single tile\n",
    "            in_specs=[\n",
    "                pl.BlockSpec(block_shape=q_tile.shape, index_map=lambda: (0, 0)),\n",
    "                pl.BlockSpec(block_shape=k_tile.shape, index_map=lambda: (0, 0)),\n",
    "                pl.BlockSpec(block_shape=v_tile.shape, index_map=lambda: (0, 0)),\n",
    "            ],\n",
    "            out_specs=[\n",
    "                pl.BlockSpec(block_shape=out_shape.shape, index_map=lambda: (0, 0)),\n",
    "                pl.BlockSpec(block_shape=u_shape.shape, index_map=lambda: (0, 0)),\n",
    "                pl.BlockSpec(block_shape=v_shape.shape, index_map=lambda: (0, 0)),\n",
    "            ],\n",
    "            **kw,\n",
    "        )(q_tile, k_tile, v_tile)\n",
    "\n",
    "    # Scan over K/V blocks for a single Q block\n",
    "    def scan_kv_blocks(acc, kv_tuple):\n",
    "        k_tile, v_tile = kv_tuple\n",
    "        q_tile, out_sum, u_list, v_list = acc\n",
    "        out_tile, u_tile, v_tile = compute_tile(q_tile, k_tile, v_tile)\n",
    "        return (q_tile, out_sum + out_tile, u_list, v_list), (u_tile, v_tile)\n",
    "\n",
    "    def process_q_block(q_tile, k_blocks_all, v_blocks_all):\n",
    "        init_sum = jnp.zeros((block_size, head_dim), dtype=q_tile.dtype)\n",
    "        \n",
    "        (q_val, final_sum, _, _), (u_stack, v_stack) = jax.lax.scan(\n",
    "            scan_kv_blocks, (q_tile, init_sum, None, None), (k_blocks_all, v_blocks_all)\n",
    "        )\n",
    "\n",
    "        final_out = final_sum / num_blocks\n",
    "\n",
    "        u_avg = jnp.mean(u_stack, axis=0)[:, 0]  # [block_size]\n",
    "        v_all = (\n",
    "            v_stack[:, :, 0]  # [num_blocks_K, block_size]\n",
    "        )\n",
    "\n",
    "        return final_out, u_avg, v_all\n",
    "\n",
    "    # Vmap across the dimensions\n",
    "    vmap_q = jax.vmap(process_q_block, in_axes=(0, None, None))\n",
    "    vmap_heads = jax.vmap(vmap_q, in_axes=(0, 0, 0))\n",
    "    vmap_batch = jax.vmap(vmap_heads, in_axes=(0, 0, 0))\n",
    "\n",
    "    # out: [B, H, num_blocks_Q, block_size, D]\n",
    "    # u:   [B, H, num_blocks_Q, block_size]\n",
    "    # v:   [B, H, num_blocks_Q, num_blocks_K, block_size]\n",
    "    out_blocks, u_blocks, v_blocks = vmap_batch(q_blocks, k_blocks, v_blocks)\n",
    "\n",
    "    out = out_blocks.reshape(B, H, L, D)\n",
    "    u_out = u_blocks.reshape(B, H, L)\n",
    "\n",
    "    # v is tricky because it was computed for every Q-block.\n",
    "    # v should be per-column (K). We should average v across Q-blocks.\n",
    "    # v_blocks is [B, H, num_blocks_Q, num_blocks_K, block_size]\n",
    "    v_avg = jnp.mean(\n",
    "        v_blocks, axis=2\n",
    "    )  # Average over Q-blocks -> [B, H, num_blocks_K, block_size]\n",
    "    v_out = v_avg.reshape(B, H, L)\n",
    "\n",
    "    return out, (u_out, v_out, Q, K, V)\n",
    "\n",
    "\n",
    "\n",
    "# Backward kernel\n",
    "def dq_kernel_batched(\n",
    "    Q_ref, K_ref, V_ref, u_ref, v_ref, dO_ref, lu_ref, lv_ref, dQ_ref\n",
    "):\n",
    "    q_block = Q_ref[...]\n",
    "    u_block = u_ref[...]\n",
    "    do_block = dO_ref[...]\n",
    "    lu_block = lu_ref[...]\n",
    "\n",
    "    D = q_block.shape[-1]\n",
    "    scale = 1.0 / jnp.sqrt(D)\n",
    "\n",
    "    N = K_ref.shape[1]\n",
    "    BLOCK_N = 128  # Safe Block Size\n",
    "    UNROLL = 4  # Speed optimization\n",
    "    iters = N // BLOCK_N\n",
    "\n",
    "    init_state = (jnp.zeros_like(q_block),)\n",
    "\n",
    "    def body_fn(i, state):\n",
    "        (dq_acc,) = state\n",
    "\n",
    "        for u_i in range(UNROLL):\n",
    "            off = (i * UNROLL + u_i) * BLOCK_N\n",
    "\n",
    "            k_block = K_ref[:, pl.ds(off, BLOCK_N), :]\n",
    "            v_pot_block = v_ref[:, pl.ds(off, BLOCK_N)]\n",
    "            v_mat_block = V_ref[:, pl.ds(off, BLOCK_N), :]\n",
    "            lv_block = lv_ref[:, pl.ds(off, BLOCK_N)]\n",
    "\n",
    "            s_block = jnp.einsum(\"bmd,bnd->bmn\", q_block, k_block) * scale\n",
    "            logits = s_block + u_block[:, :, None] + v_pot_block[:, None, :]\n",
    "            a_block = jnp.exp(logits)\n",
    "\n",
    "            z_block = jnp.einsum(\"bme,bne->bmn\", do_block, v_mat_block)\n",
    "            term_block = z_block - lu_block[:, :, None] - lv_block[:, None, :]\n",
    "            ds_block = a_block * term_block\n",
    "\n",
    "            dq_acc = dq_acc + jnp.einsum(\"bmn,bnd->bmd\", ds_block, k_block)\n",
    "\n",
    "        return (dq_acc,)\n",
    "\n",
    "    res = jax.lax.fori_loop(0, iters // UNROLL, body_fn, init_state)\n",
    "    dq_acc = res[0]\n",
    "\n",
    "    dQ_ref[...] = dq_acc * scale\n",
    "\n",
    "\n",
    "def dk_dv_kernel_batched(\n",
    "    Q_ref, K_ref, V_ref, u_ref, v_ref, dO_ref, lu_ref, lv_ref, dK_ref, dV_ref\n",
    "):\n",
    "    k_block = K_ref[...]\n",
    "    v_pot_block = v_ref[...]\n",
    "    v_mat_block = V_ref[...]\n",
    "    lv_block = lv_ref[...]\n",
    "\n",
    "    D = k_block.shape[-1]\n",
    "    M = Q_ref.shape[1]\n",
    "    BLOCK_M = 128  # Safe Block Size\n",
    "    UNROLL = 4  # Speed optimization\n",
    "    iters = M // BLOCK_M\n",
    "    scale = 1.0 / jnp.sqrt(D)\n",
    "\n",
    "    init_state = (jnp.zeros_like(k_block), jnp.zeros_like(v_mat_block))\n",
    "\n",
    "    def body_fn(i, state):\n",
    "        dk_acc, dv_acc = state\n",
    "\n",
    "        for u_i in range(UNROLL):\n",
    "            off = (i * UNROLL + u_i) * BLOCK_M\n",
    "\n",
    "            q_block = Q_ref[:, pl.ds(off, BLOCK_M), :]\n",
    "            u_block = u_ref[:, pl.ds(off, BLOCK_M)]\n",
    "            do_block = dO_ref[:, pl.ds(off, BLOCK_M), :]\n",
    "            lu_block = lu_ref[:, pl.ds(off, BLOCK_M)]\n",
    "\n",
    "            s_block = jnp.einsum(\"bmd,bnd->bmn\", q_block, k_block) * scale\n",
    "            logits = s_block + u_block[:, :, None] + v_pot_block[:, None, :]\n",
    "            a_block = jnp.exp(logits)\n",
    "\n",
    "            z_block = jnp.einsum(\"bme,bne->bmn\", do_block, v_mat_block)\n",
    "            term_block = z_block - lu_block[:, :, None] - lv_block[:, None, :]\n",
    "            ds_block = a_block * term_block\n",
    "\n",
    "            dk_acc = dk_acc + jnp.einsum(\n",
    "                \"bnm,bmd->bnd\", ds_block.transpose(0, 2, 1), q_block\n",
    "            )\n",
    "            dv_acc = dv_acc + jnp.einsum(\n",
    "                \"bnm,bme->bne\", a_block.transpose(0, 2, 1), do_block\n",
    "            )\n",
    "\n",
    "        return (dk_acc, dv_acc)\n",
    "\n",
    "    res = jax.lax.fori_loop(0, iters // UNROLL, body_fn, init_state)\n",
    "    dk_acc, dv_acc = res\n",
    "\n",
    "    dK_ref[...] = dk_acc * scale\n",
    "    dV_ref[...] = dv_acc\n",
    "\n",
    "\n",
    "def pallas_sinkhorn_bwd_fused(u, v, Q, K, V, dO, lambda_u, lambda_v):\n",
    "    B, H, M, D = Q.shape\n",
    "    _, _, N, E = V.shape\n",
    "\n",
    "    Batch = B * H\n",
    "    Q_f = Q.reshape(Batch, M, D)\n",
    "    K_f = K.reshape(Batch, N, D)\n",
    "    V_f = V.reshape(Batch, N, E)\n",
    "    u_f = u.reshape(Batch, M)\n",
    "    v_f = v.reshape(Batch, N)\n",
    "    dO_f = dO.reshape(Batch, M, E)\n",
    "    lu_f = lambda_u.reshape(Batch, M)\n",
    "    lv_f = lambda_v.reshape(Batch, N)\n",
    "\n",
    "    BLOCK = 128  # Safe Block Size\n",
    "    BATCH_BLOCK = 8  # Safe Batch Block\n",
    "\n",
    "    is_cpu = jax.devices()[0].platform == \"cpu\"\n",
    "    kw = (\n",
    "        {\"interpret\": True}\n",
    "        if is_cpu\n",
    "        else {\"compiler_params\": resolve_compiler_params((\"parallel\", \"parallel\"))}\n",
    "    )\n",
    "\n",
    "    grid_dq = (Batch // BATCH_BLOCK, M // BLOCK)\n",
    "\n",
    "    dQ_f = pl.pallas_call(\n",
    "        dq_kernel_batched,\n",
    "        out_shape=jax.ShapeDtypeStruct(Q_f.shape, Q_f.dtype),\n",
    "        grid=grid_dq,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, m, 0), block_shape=(BATCH_BLOCK, BLOCK, D)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, 0, 0), block_shape=(BATCH_BLOCK, N, D)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, 0, 0), block_shape=(BATCH_BLOCK, N, E)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, m), block_shape=(BATCH_BLOCK, BLOCK)\n",
    "            ),\n",
    "            pl.BlockSpec(index_map=lambda b, m: (b, 0), block_shape=(BATCH_BLOCK, N)),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, m, 0), block_shape=(BATCH_BLOCK, BLOCK, E)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, m: (b, m), block_shape=(BATCH_BLOCK, BLOCK)\n",
    "            ),\n",
    "            pl.BlockSpec(index_map=lambda b, m: (b, 0), block_shape=(BATCH_BLOCK, N)),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec(\n",
    "            index_map=lambda b, m: (b, m, 0), block_shape=(BATCH_BLOCK, BLOCK, D)\n",
    "        ),\n",
    "        **kw,\n",
    "    )(Q_f, K_f, V_f, u_f, v_f, dO_f, lu_f, lv_f)\n",
    "\n",
    "    grid_dk_dv = (Batch // BATCH_BLOCK, N // BLOCK)\n",
    "\n",
    "    out_shape_k = jax.ShapeDtypeStruct(K_f.shape, K_f.dtype)\n",
    "    out_shape_v = jax.ShapeDtypeStruct(V_f.shape, V_f.dtype)\n",
    "\n",
    "    dK_f, dV_f = pl.pallas_call(\n",
    "        dk_dv_kernel_batched,\n",
    "        out_shape=(out_shape_k, out_shape_v),\n",
    "        grid=grid_dk_dv,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, 0, 0), block_shape=(BATCH_BLOCK, M, D)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n, 0), block_shape=(BATCH_BLOCK, BLOCK, D)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n, 0), block_shape=(BATCH_BLOCK, BLOCK, E)\n",
    "            ),\n",
    "            pl.BlockSpec(index_map=lambda b, n: (b, 0), block_shape=(BATCH_BLOCK, M)),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n), block_shape=(BATCH_BLOCK, BLOCK)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, 0, 0), block_shape=(BATCH_BLOCK, M, E)\n",
    "            ),\n",
    "            pl.BlockSpec(index_map=lambda b, n: (b, 0), block_shape=(BATCH_BLOCK, M)),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n), block_shape=(BATCH_BLOCK, BLOCK)\n",
    "            ),\n",
    "        ],\n",
    "        out_specs=[\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n, 0), block_shape=(BATCH_BLOCK, BLOCK, D)\n",
    "            ),\n",
    "            pl.BlockSpec(\n",
    "                index_map=lambda b, n: (b, n, 0), block_shape=(BATCH_BLOCK, BLOCK, E)\n",
    "            ),\n",
    "        ],\n",
    "        **kw,\n",
    "    )(Q_f, K_f, V_f, u_f, v_f, dO_f, lu_f, lv_f)\n",
    "\n",
    "    return dQ_f.reshape(Q.shape), dK_f.reshape(K.shape), dV_f.reshape(V.shape)\n",
    "\n",
    "\n",
    "def pallas_sinkhorn_bwd_full(n_iters, res, g):\n",
    "    u, v, Q, K, V = res\n",
    "    dO = g\n",
    "\n",
    "    # Constants\n",
    "    scale = 1.0 / jnp.sqrt(Q.shape[-1])\n",
    "    BLOCK = 128\n",
    "    M, N = Q.shape[-2], K.shape[-2]\n",
    "\n",
    "    g_u_init = jnp.zeros_like(u)\n",
    "    g_v_init = jnp.zeros_like(v)\n",
    "\n",
    "    def pass1_body(carry, i):\n",
    "        g_u, g_v = carry\n",
    "\n",
    "        q_block = jax.lax.dynamic_slice_in_dim(Q, i * BLOCK, BLOCK, axis=-2)\n",
    "        u_block = jax.lax.dynamic_slice_in_dim(u, i * BLOCK, BLOCK, axis=-1)\n",
    "        do_block = jax.lax.dynamic_slice_in_dim(dO, i * BLOCK, BLOCK, axis=-2)\n",
    "\n",
    "        Z_block = jnp.matmul(do_block, V.transpose(0, 1, 3, 2))\n",
    "        S_block = jnp.einsum(\"bhmd,bhnd->bhmn\", q_block, K) * scale\n",
    "        A_block = jnp.exp(S_block + u_block[..., :, None] + v[..., None, :])\n",
    "        T_block = A_block * Z_block\n",
    "\n",
    "        gu_contrib = jnp.sum(T_block, axis=-1)\n",
    "        gv_contrib = jnp.sum(T_block, axis=-2)\n",
    "\n",
    "        g_u = jax.lax.dynamic_update_slice_in_dim(g_u, gu_contrib, i * BLOCK, axis=-1)\n",
    "        g_v = g_v + gv_contrib\n",
    "\n",
    "        return (g_u, g_v), None\n",
    "\n",
    "    (g_u, g_v), _ = jax.lax.scan(\n",
    "        pass1_body, (g_u_init, g_v_init), jnp.arange(M // BLOCK)\n",
    "    )\n",
    "\n",
    "    lambda_u = jnp.zeros_like(u)\n",
    "    lambda_v = jnp.zeros_like(v)\n",
    "\n",
    "    def matvec_A(vec_v):\n",
    "        def mv_body(carry, i):\n",
    "            q_block = jax.lax.dynamic_slice_in_dim(Q, i * BLOCK, BLOCK, axis=-2)\n",
    "            u_block = jax.lax.dynamic_slice_in_dim(u, i * BLOCK, BLOCK, axis=-1)\n",
    "            S_block = jnp.einsum(\"bhmd,bhnd->bhmn\", q_block, K) * scale\n",
    "            A_block = jnp.exp(S_block + u_block[..., :, None] + v[..., None, :])\n",
    "            res = A_block @ vec_v[..., None]\n",
    "            return carry, res.squeeze(-1)\n",
    "\n",
    "        _, res = jax.lax.scan(mv_body, None, jnp.arange(M // BLOCK))\n",
    "        return jnp.concatenate(res, axis=-1)\n",
    "\n",
    "    def matvec_AT(vec_u):\n",
    "        res = jnp.zeros_like(v)\n",
    "\n",
    "        def mvt_body(acc, i):\n",
    "            q_block = jax.lax.dynamic_slice_in_dim(Q, i * BLOCK, BLOCK, axis=-2)\n",
    "            u_block = jax.lax.dynamic_slice_in_dim(u, i * BLOCK, BLOCK, axis=-1)\n",
    "            vec_u_block = jax.lax.dynamic_slice_in_dim(vec_u, i * BLOCK, BLOCK, axis=-1)\n",
    "            S_block = jnp.einsum(\"bhmd,bhnd->bhmn\", q_block, K) * scale\n",
    "            A_block = jnp.exp(S_block + u_block[..., :, None] + v[..., None, :])\n",
    "            contrib = jnp.matmul(\n",
    "                A_block.transpose(0, 1, 3, 2), vec_u_block[..., None]\n",
    "            ).squeeze(-1)\n",
    "            return acc + contrib, None\n",
    "\n",
    "        res, _ = jax.lax.scan(mvt_body, res, jnp.arange(M // BLOCK))\n",
    "        return res\n",
    "\n",
    "    def adjoint_step(i, state):\n",
    "        lu, lv = state\n",
    "        lu = g_u - matvec_A(lv)\n",
    "        lv = g_v - matvec_AT(lu)\n",
    "        return (lu, lv)\n",
    "\n",
    "    lambda_u, lambda_v = jax.lax.fori_loop(0, 10, adjoint_step, (lambda_u, lambda_v))\n",
    "\n",
    "    return pallas_sinkhorn_bwd_fused(u, v, Q, K, V, dO, lambda_u, lambda_v)\n",
    "\n",
    "\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(3,))\n",
    "def pallas_flash_sinkhorn(Q, K, V, n_iters=20):\n",
    "    # Use the PURE PALLAS forward pass\n",
    "    O, _ = pallas_sinkhorn_fwd(Q, K, V, n_iters)\n",
    "    return O\n",
    "\n",
    "\n",
    "# Bind the Pallas-accelerated backward pass\n",
    "pallas_flash_sinkhorn.defvjp(pallas_sinkhorn_fwd, pallas_sinkhorn_bwd_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0266bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Pallas Sinkhorn...\n",
      "Output Shape: (1, 4, 1024, 128)\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "try:\n",
    "    from pallas_sinkhorn import pallas_flash_sinkhorn\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    B, H, L, D = 1, 4, 1024, 128\n",
    "    Q = jax.random.normal(key, (B, H, L, D))\n",
    "    K = jax.random.normal(key, (B, H, L, D))\n",
    "    V = jax.random.normal(key, (B, H, L, D))\n",
    "\n",
    "    print(\"Compiling Pallas Sinkhorn...\")\n",
    "    out = jax.block_until_ready(pallas_flash_sinkhorn(Q, K, V))\n",
    "    print(\"Output Shape:\", out.shape)\n",
    "    print(\"Success!\")\n",
    "except Exception as e:\n",
    "    print(\"Verification Failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
